# -*- coding: utf-8 -*-
"""Fin Detection Model 3o

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bdglvmZpyACmkfWvVLBzHw4b2v_Oe7wh
"""

import os
import zipfile

from google.colab import files

# this will pop up a widget‚Äîselect your .zip from your local machine
uploaded = files.upload()

# grab its filename automatically
zip_path = list(uploaded.keys())[0]
print("Using:", zip_path)

extract_path = "/content/shark_dataset"
os.makedirs(extract_path, exist_ok=True)
with zipfile.ZipFile(zip_path, "r") as z:
    z.extractall(extract_path)

!ls "/content/finning final.v5i.yolov8.zip"

!find /content/shark_dataset -maxdepth 2 -type d

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# cat <<EOF > /content/shark_dataset/data.yaml
# path: /content/shark_dataset
# train: train/images
# val:   valid/images
# test:  test/images
# 
# nc: 1
# names:
#   0: shark_finning
# EOF
#

from google.colab import drive
drive.mount('/content/drive')

!pip install -q ultralytics roboflow

from ultralytics import YOLO

# pick a base model (tiny for speed, small/medium for more accuracy)
model = YOLO('/content/runs/detect/shark_finning_run/weights/best.pt')

model.train(
    data='/content/shark_dataset/data.yaml',
    epochs=50,
    imgsz=640,
    batch=16,
    name='shark_finning_run'
)

!ls /content/runs/detect/shark_finning_run/weights

!ls /content/runs/detect/shark_finning_run2/weights/best.pt

# ‚îÄ‚îÄ DEMO: Fake AIS insights + YOLOv8 pipeline ‚îÄ‚îÄ

from ultralytics import YOLO
import random
from datetime import datetime, timedelta
import glob, random

# 1) Create fake AIS ‚Äúevents‚Äù for 5 vessels over a 1-hour window
VIDEO_START = datetime(2025,6,21,12,0,0)
ais_events = []
for i in range(5):
    event_time = VIDEO_START + timedelta(minutes= i*10 + random.randint(0,5))
    ais_events.append({
        "mmsi":       100000000 + i,
        "shipname":   f"Vessel_{i}",
        "gearType":   random.choice(["TRAWLER","LONG_LINE","PURSE_SEINE"]),
        "lat":        24.6  + random.random()*0.5,
        "lon":       -80.4  + random.random()*0.5,
        "timestamp":  event_time.isoformat()+"Z"
    })

print("‚úÖ Fake AIS events:")
for e in ais_events:
    print(" ", e)



# ‚Üê catch train, valid and item/test all in one go
IMAGE_PATHS = glob.glob("/content/finning_dataset/*/images/*.jpg")

def fetch_sat_image(lat, lon, ts):
    # ignore lat/lon/time‚Äîjust pick a random real frame
    return random.choice(IMAGE_PATHS)

# 3) Load your trained YOLOv8 model
model = YOLO("/content/best2.pt")


# 4) Run inference on each ‚Äúevent‚Äù frame and merge with fake AIS metadata
print("\nüöÄ Running YOLO on each fake event:")
for ev in ais_events:
    img = fetch_sat_image(ev["lat"], ev["lon"], ev["timestamp"])
    results = model.predict(source=img, conf=0.3, verbose=False)
    for ev in ais_events:
        img = fetch_sat_image(ev["lat"], ev["lon"], ev["timestamp"])
        results = model.predict(source=img, conf=0.3, verbose=False)
        for r in results:
            boxes = r.boxes.xyxy.tolist()       # list of [x1,y1,x2,y2]
            confs = r.boxes.conf.tolist()       # list of floats
            for bbox, confidence in zip(boxes, confs):
                out = {
                    "frame_time": ev["timestamp"],
                    "mmsi":       ev["mmsi"],
                    "shipname":   ev["shipname"],
                    "gearType":   ev["gearType"],
                    "bbox":       bbox,
                    "confidence": confidence,
                }
                print(out)


import json

# 1) Prepare an empty list to hold all detections
all_detections = []

# 2) Run your pipeline and append each detection to the list
for ev in ais_events:
    img = fetch_sat_image(ev["lat"], ev["lon"], ev["timestamp"])
    results = model.predict(source=img, conf=0.3, verbose=False)
    for r in results:
        boxes = r.boxes.xyxy.tolist()
        confs = r.boxes.conf.tolist()
        for bbox, confidence in zip(boxes, confs):
            out = {
                "frame_time": ev["timestamp"],
                "mmsi":       ev["mmsi"],
                "shipname":   ev["shipname"],
                "gearType":   ev["gearType"],
                "bbox":       bbox,
                "confidence": confidence,
            }
            all_detections.append(out)

# 3) Write the list out to a JSON file
json_path = "/content/detections.json"
with open(json_path, "w") as f:
    json.dump(all_detections, f, indent=2)

print(f"‚úÖ Saved {len(all_detections)} detections to {json_path}")

import os

for root, dirs, files in os.walk("/content"):
    for f in files:
        if f.endswith(".pt"):
            print(os.path.join(root, f))

# If the ZIP is literally named best (2).pt.zip in the root:
!unzip "/best (2).pt" -d /content

import os
print(os.listdir("/content"))
# you should see "best (2).pt" listed

# show everything in the root
!ls -al /

# show everything in /content
!ls -al /content

# search deeply for any .pt files under /content
!find /content -type f -iname "*.pt"

# move from root into /content and give it a clean name
!mv "/best (2).pt" /content/best2.pt

!ls /content | grep best2.pt

# in a new Colab cell
!mv "/finning final.v5i.yolov8.zip" /content/finning.zip
!unzip /content/finning.zip -d /content/finning_dataset